{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2yOrdQr0jRsE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYEEF8BYGqAk"
      },
      "outputs": [],
      "source": [
        "!pip install scikeras[tensorflow]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import itertools\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import make_scorer\n",
        "import concurrent.futures\n",
        "from threading import Lock\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.linear_model import RidgeCV\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from scikeras.wrappers import KerasRegressor"
      ],
      "metadata": {
        "id": "DHR6c1wW1y3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "L-v4zOWAUNNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgkD0MB8d4ix"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing"
      ],
      "metadata": {
        "id": "2yOrdQr0jRsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the CUP dataset"
      ],
      "metadata": {
        "id": "TKkes4mNrNK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "header=['ID','a','b','c','d','e','f', 'g', 'h', 'i', 'j','Class_x', 'Class_y']\n",
        "df = pd.read_csv(\"/content/drive/Shareddrives/ML/CUP/ML-CUP21-TR.csv\", header=None,delimiter=',', skiprows=7,names=header)\n",
        "df.index=df['ID'].values\n",
        "df.drop('ID', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "JGqf46Kdi9Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "jygDUftQjV_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "kg86-v2-nGKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col=[c for c in df.columns if (c!='Class_x' and c!= 'Class_y')]\n",
        "x= df[col].values\n",
        "y= df[['Class_x', 'Class_y']].values"
      ],
      "metadata": {
        "id": "BottViv9jl_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training/Test splitting with Hold-out approch (90%-10%)"
      ],
      "metadata": {
        "id": "jD3bJU_6rSUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=0)\n",
        "print(y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "oQ2hWc5Dkjdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MEE definition"
      ],
      "metadata": {
        "id": "8qO1qA03ril1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_euclidean_error_tf(y_true, y_pred):\n",
        "    return K.mean(K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1)))"
      ],
      "metadata": {
        "id": "vbFgFShsmAZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_euclidean_error(y_true, y_pred):\n",
        "    return np.mean(np.sqrt(np.sum(np.square(y_pred-y_true), axis=-1)))"
      ],
      "metadata": {
        "id": "m9s3Ztrw4fOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = make_scorer(mean_euclidean_error, greater_is_better = False)"
      ],
      "metadata": {
        "id": "84paq2qQXi1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDXXA6sARPhN"
      },
      "outputs": [],
      "source": [
        "def build_model(weight_init=0.2, weight_distr=0, activ='relu',layer=1, unit=4, eta=0.2, alpha=0.5, lambd=0):\n",
        "  \n",
        "  tf.random.set_seed(0)  \n",
        "    \n",
        "  if weight_distr==0:\n",
        "    init= tf.keras.initializers.RandomUniform(minval=-weight_init, maxval=weight_init)\n",
        "  elif weight_distr==1:\n",
        "    init= tf.keras.initializers.RandomNormal(mean=0., stddev=weight_init)\n",
        "  else:\n",
        "    init= tf.keras.initializers.GlorotNormal()\n",
        "\n",
        "  reg= tf.keras.regularizers.l2(l2=lambd)\n",
        "\n",
        "\n",
        "  model= tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Input(10,))\n",
        "  for i in range(layer):\n",
        "    model.add(tf.keras.layers.Dense(unit, activation='tanh', kernel_initializer=init, bias_initializer=init, kernel_regularizer=reg))\n",
        "  model.add(tf.keras.layers.Dense(2, activation='linear', kernel_initializer=init, bias_initializer=init, kernel_regularizer=reg))\n",
        "\n",
        "  loss=mean_euclidean_error_tf\n",
        "  opt= tf.keras.optimizers.SGD(learning_rate=eta, momentum=alpha, nesterov=False)\n",
        "  metric=mean_euclidean_error_tf\n",
        "  model.compile(loss=loss, \n",
        "                optimizer=opt,\n",
        "                metrics=[metric])\n",
        "  \n",
        "  #print(model.get_weights())\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model_Adam(weight_init=0.2, weight_distr=0, activ='relu',layer=1, unit=4, eta=0.2, alpha=0.5, lambd=0, beta_1=0.9, beta_2=0.999, epsilon=0.0000001):\n",
        "  \n",
        "  tf.random.set_seed(0)  \n",
        "    \n",
        "  if weight_distr==0:\n",
        "    init= tf.keras.initializers.RandomUniform(minval=-weight_init, maxval=weight_init)\n",
        "  elif weight_distr==1:\n",
        "    init= tf.keras.initializers.RandomNormal(mean=0., stddev=weight_init)\n",
        "  else:\n",
        "    init= tf.keras.initializers.GlorotNormal()\n",
        "\n",
        "  reg= tf.keras.regularizers.l2(l2=lambd)\n",
        "\n",
        "\n",
        "  model= tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.Input(10,))\n",
        "  for i in range(layer):\n",
        "    model.add(tf.keras.layers.Dense(unit, activation='tanh', kernel_initializer=init, bias_initializer=init, kernel_regularizer=reg))\n",
        "  model.add(tf.keras.layers.Dense(2, activation='linear', kernel_initializer=init, bias_initializer=init, kernel_regularizer=reg))\n",
        "\n",
        "  loss=mean_euclidean_error_tf\n",
        "  opt= tf.keras.optimizers.Adam(learning_rate=eta, beta_1=beta_1, beta_2=beta_2,epsilon=epsilon)\n",
        "  metric=mean_euclidean_error_tf\n",
        "  model.compile(loss=loss, \n",
        "                optimizer=opt,\n",
        "                metrics=[metric])\n",
        "  \n",
        "  #print(model.get_weights())\n",
        "  return model"
      ],
      "metadata": {
        "id": "R38lyUmjJ6jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class haltCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, err):\n",
        "        super(tf.keras.callbacks.Callback, self).__init__()\n",
        "        self.err=err\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('mean_euclidean_error_tf') <=self.err):\n",
        "            self.model.stop_training = True"
      ],
      "metadata": {
        "id": "hJG74vLDLPdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stacking"
      ],
      "metadata": {
        "id": "WgG6iudDxl8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble of base estimators with final Ridge regressor on top of their predictions"
      ],
      "metadata": {
        "id": "jDa0PXzoz3zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Stacking():\n",
        "\n",
        "  #Stacking constructor which takes the best grid's output hyper-parameters for\n",
        "  #the estimators to ensamble\n",
        "  def __init__(self) -> None:\n",
        "      nn_param_batch={\n",
        "          'weight_init': 0.4,\n",
        "          'weight_distr': 1,\n",
        "          'unit': 40,\n",
        "          'layer': 3,\n",
        "          'eta': 0.03,\n",
        "          'alpha': 0.95,\n",
        "          'lambd': 0.001,\n",
        "          'activ': 'tanh',\n",
        "      }\n",
        "      mean_err_batch= 0.8342830985784531  #tr\n",
        "      trainingStopCallback_batch = haltCallback(mean_err_batch)\n",
        "\n",
        "      nn_param_mb={\n",
        "          'weight_init': 0.2,\n",
        "          'weight_distr': 1,\n",
        "          'unit': 50,\n",
        "          'layer': 3,\n",
        "          'eta': 0.025,\n",
        "          'alpha': 0,\n",
        "          'lambd': 0.0005,\n",
        "          'activ': 'tanh'\n",
        "      }\n",
        "      mean_err_mb= 1.0076722502708435\n",
        "      trainingStopCallback_mb = haltCallback(mean_err_mb)\n",
        "\n",
        "\n",
        "      nn_param_adam={\n",
        "          'weight_init': 0.4,\n",
        "          'weight_distr': 1,\n",
        "          'unit': 40,\n",
        "          'layer': 3,\n",
        "          'eta': 0.025,\n",
        "          'beta_1': 0.9,\n",
        "          'beta_2': 0.7,\n",
        "          'lambd': 0.001,\n",
        "          'activ': 'tanh'\n",
        "      }\n",
        "      mean_err_adam= 0.8765210807323456\n",
        "      trainingStopCallback_adam = haltCallback(mean_err_adam)\n",
        "\n",
        "\n",
        "      nn_param_online={\n",
        "          'weight_init': 0.2,\n",
        "          'weight_distr': 1,\n",
        "          'unit': 50,\n",
        "          'eta': 0.0005,\n",
        "          'alpha': 0,\n",
        "          'layer': 2,\n",
        "          'lambd': 0.001,\n",
        "          'activ': 'tanh'\n",
        "      }\n",
        "      mean_err_online= 0.9664344638586044\n",
        "      trainingStopCallback_online = haltCallback(mean_err_online)\n",
        "\n",
        "      #estimator definition with best hyper-parameters\n",
        "      self.estimators=[\n",
        "            ('knn', KNeighborsRegressor(n_neighbors= 9, p=1, weights='distance')),\n",
        "            #('svm', MultiOutputRegressor(SVR(C= 10, epsilon= 0.5, gamma= 'auto', kernel= 'rbf'))),\n",
        "            #('lbe', Pipeline(steps=[('lbe',PolynomialFeatures(degree=4, interaction_only=True)),('ridge',Ridge(alpha=10, solver='saga', random_state=0))])),\n",
        "            ('batch', KerasRegressor(build_model, **nn_param_batch, epochs=800, shuffle=True, batch_size=len(x_train), verbose=0, callbacks=[trainingStopCallback_batch])),\n",
        "            #('mb', KerasRegressor(build_model, **nn_param_mb, epochs=800, shuffle=True, batch_size=100, verbose=0, callbacks=[trainingStopCallback_mb])),\n",
        "            ('adam', KerasRegressor(build_model_Adam, **nn_param_adam, epochs=800, shuffle=True, batch_size=len(x_train), verbose=0, callbacks=[trainingStopCallback_adam])),\n",
        "            #('online', KerasRegressor(build_model, **nn_param_online, epochs=400, shuffle=True, batch_size=1, verbose=0, callbacks=[trainingStopCallback_online])),\n",
        "            ('forest', RandomForestRegressor(max_depth=20, random_state=0, n_estimators=100, criterion='squared_error', min_samples_split=2, min_samples_leaf=2, \n",
        "                                    max_features = 3, bootstrap=False))\n",
        "      ]\n",
        "\n",
        "      #Ridge as final estimator on top of the base estimators' outputs\n",
        "      #the RidgeCV class has a built-in grid-search in order to tune the alpha parameter\n",
        "      self.final_estimator_x= RidgeCV(alphas=(0.001,0.01,0.1,1,10,25,50,75,100,200), scoring='neg_mean_squared_error', cv=5)\n",
        "      self.final_estimator_y= RidgeCV(alphas=(0.001,0.01,0.1,1,10,50,75,100,150,200,250,300), scoring='neg_mean_squared_error', cv=5)\n",
        "      \n",
        "      self.col= [name for (name, estimator) in self.estimators]\n",
        "\n",
        "  #In the fit method we first train each base estimator with a cross validated approch.\n",
        "  #Then we build a DataFrame made by base estimator predictions on which we tune our Ridge models (one for each target x/y)\n",
        "  def fit(self, X_train, y_train):\n",
        "      pred_train_x= pd.DataFrame(columns=self.col)\n",
        "      pred_train_y= pd.DataFrame(columns=self.col)\n",
        "\n",
        "      for (name, estimator) in self.estimators:\n",
        "        pred= cross_val_predict(estimator, X_train, y_train, cv=4)\n",
        "        pred_train_x[name]= pred[:,0]\n",
        "        pred_train_y[name]= pred[:,1]\n",
        "\n",
        "      val_mee=[]\n",
        "      kf = KFold(n_splits=4, shuffle=True, random_state=0)\n",
        "      #############!!!!!!!!!!!###########\n",
        "      for train, val in kf.split(pred_train_x):\n",
        "        self.final_estimator_x.fit(pred_train_x.loc[train], y_train[train,0]) #chiamaRidgeCV\n",
        "        pred_val_x= self.final_estimator_x.predict(pred_train_x.loc[val])\n",
        "\n",
        "        self.final_estimator_y.fit(pred_train_y.loc[train], y_train[train,1])\n",
        "        pred_val_y= self.final_estimator_y.predict(pred_train_y.loc[val])\n",
        "\n",
        "        print(self.final_estimator_x.alpha_, self.final_estimator_y.alpha_)\n",
        "        print(mean_euclidean_error(y_train[val], np.column_stack((pred_val_x, pred_val_y))))\n",
        "        val_mee.append(mean_euclidean_error(y_train[val], np.column_stack((pred_val_x, pred_val_y))))\n",
        "        \n",
        "      self.final_estimator_x.fit(pred_train_x, y_train[:,0])\n",
        "      self.final_estimator_y.fit(pred_train_y, y_train[:,1])\n",
        "\n",
        "      return (np.mean(val_mee), np.std(val_mee), self.final_estimator_x.alpha_, self.final_estimator_y.alpha_)\n",
        "\n",
        "  #In the predict method we first retrain the base estimators on the whole TR\n",
        "  #And then we use the trained models to predict the TS, building the DataFrame on which Ridge predicts the final outputs\n",
        "  def predict(self, X_train, y_train, X_test):\n",
        "      pred_test_x= pd.DataFrame(columns=self.col)\n",
        "      pred_test_y= pd.DataFrame(columns=self.col)\n",
        "\n",
        "      for (name, estimator) in self.estimators:\n",
        "        estimator.fit(X_train, y_train)\n",
        "        pred_test= estimator.predict(X_test)\n",
        "        pred_test_x[name]= pred_test[:,0]\n",
        "        pred_test_y[name]= pred_test[:,1]\n",
        "\n",
        "      return np.column_stack((self.final_estimator_x.predict(pred_test_x), self.final_estimator_y.predict(pred_test_y)))"
      ],
      "metadata": {
        "id": "hjU6fxQLB5OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Stacking():\n",
        "\n",
        "  #Stacking constructor which takes the best grid's output hyper-parameters for\n",
        "  #the estimators to ensamble\n",
        "  def __init__(self) -> None:\n",
        "      nn_param_batch={\n",
        "          'weight_init': 0.4,\n",
        "          'weight_distr': 1,\n",
        "          'unit': 40,\n",
        "          'layer': 3,\n",
        "          'eta': 0.03,\n",
        "          'alpha': 0.95,\n",
        "          'lambd': 0.001,\n",
        "          'activ': 'tanh',\n",
        "      }\n",
        "      mean_err_batch= 0.8342830985784531\n",
        "      trainingStopCallback_batch = haltCallback(mean_err_batch)\n",
        "\n",
        "      nn_param_mb={\n",
        "          'weight_init': 0.2,\n",
        "          'weight_distr': 1,\n",
        "          'unit': 50,\n",
        "          'layer': 3,\n",
        "          'eta': 0.025,\n",
        "          'alpha': 0,\n",
        "          'lambd': 0.0005,\n",
        "          'activ': 'tanh'\n",
        "      }\n",
        "      mean_err_mb= 1.0076722502708435\n",
        "      trainingStopCallback_mb = haltCallback(mean_err_mb)\n",
        "\n",
        "\n",
        "      nn_param_adam={\n",
        "          'weight_init': 0.4,\n",
        "          'weight_distr': 1,\n",
        "          'unit': 40,\n",
        "          'layer': 3,\n",
        "          'eta': 0.025,\n",
        "          'beta_1': 0.9,\n",
        "          'beta_2': 0.7,\n",
        "          'lambd': 0.001,\n",
        "          'activ': 'tanh'\n",
        "      }\n",
        "      mean_err_adam= 0.8765210807323456\n",
        "      trainingStopCallback_adam = haltCallback(mean_err_adam)\n",
        "\n",
        "\n",
        "      nn_param_online={\n",
        "          'weight_init': 0.2,\n",
        "          'weight_distr': 1,\n",
        "          'unit': 50,\n",
        "          'eta': 0.0005,\n",
        "          'alpha': 0,\n",
        "          'layer': 2,\n",
        "          'lambd': 0.001,\n",
        "          'activ': 'tanh'\n",
        "      }\n",
        "      mean_err_online= 0.9664344638586044\n",
        "      trainingStopCallback_online = haltCallback(mean_err_online)\n",
        "\n",
        "      #estimator definition with best hyper-parameters\n",
        "      self.estimators=[\n",
        "            ('knn', KNeighborsRegressor(n_neighbors= 9, p=1, weights='distance')),\n",
        "            #('svm', MultiOutputRegressor(SVR(C= 10, epsilon= 0.5, gamma= 'auto', kernel= 'rbf'))),\n",
        "            #('lbe', Pipeline(steps=[('lbe',PolynomialFeatures(degree=4, interaction_only=True)),('ridge',Ridge(alpha=10, solver='saga', random_state=0))])),\n",
        "            ('batch', KerasRegressor(build_model, **nn_param_batch, epochs=800, shuffle=True, batch_size=len(x_train), verbose=0, callbacks=[trainingStopCallback_batch])),\n",
        "            #('mb', KerasRegressor(build_model, **nn_param_mb, epochs=800, shuffle=True, batch_size=100, verbose=0, callbacks=[trainingStopCallback_mb])),\n",
        "            ('adam', KerasRegressor(build_model_Adam, **nn_param_adam, epochs=800, shuffle=True, batch_size=len(x_train), verbose=0, callbacks=[trainingStopCallback_adam])),\n",
        "            #('online', KerasRegressor(build_model, **nn_param_online, epochs=400, shuffle=True, batch_size=1, verbose=0, callbacks=[trainingStopCallback_online])),\n",
        "            ('forest', RandomForestRegressor(max_depth=20, random_state=0, n_estimators=100, criterion='squared_error', min_samples_split=2, min_samples_leaf=2, \n",
        "                                    max_features = 3, bootstrap=False))\n",
        "      ]\n",
        "\n",
        "      #Ridge as final estimator on top of the base estimators' outputs\n",
        "      #the RidgeCV class has a built-in grid-search in order to tune the alpha parameter\n",
        "      self.final_estimator_x= RidgeCV(alphas=(0.001,0.01,0.1,1,10,25,50,75,100,200), scoring='neg_mean_squared_error', cv=5)\n",
        "      self.final_estimator_y= RidgeCV(alphas=(0.001,0.01,0.1,1,10,50,75,100,150,200,250,300), scoring='neg_mean_squared_error', cv=5)\n",
        "      \n",
        "      self.col= [name for (name, estimator) in self.estimators]\n",
        "\n",
        "  #In the fit method we first train each base estimator with a cross validated approch.\n",
        "  #Then we build a DataFrame made by base estimator predictions on which we tune our Ridge models (one for each target x/y)\n",
        "  def fit(self, X_train, y_train):\n",
        "      pred_train_x= pd.DataFrame(columns=self.col)\n",
        "      pred_train_y= pd.DataFrame(columns=self.col)\n",
        "\n",
        "      for (name, estimator) in self.estimators:\n",
        "        pred= cross_val_predict(estimator, X_train, y_train, cv=4)\n",
        "        pred_train_x[name]= pred[:,0]\n",
        "        pred_train_y[name]= pred[:,1]\n",
        "\n",
        "      self.final_estimator_x.fit(pred_train_x, y_train[:,0])  #find best alpha and retrain through RidgeCV\n",
        "      self.final_estimator_y.fit(pred_train_y, y_train[:,1])\n",
        "\n",
        "      val_mee=[]  \n",
        "      ridge_x= Ridge(alpha=self.final_estimator_x.alpha_)   #imposto alpha\n",
        "      ridge_y= Ridge(alpha=self.final_estimator_y.alpha_)\n",
        "      kf = KFold(n_splits=4, shuffle=True, random_state=0)\n",
        "      for train, val in kf.split(pred_train_x):\n",
        "        ridge_x.fit(pred_train_x.loc[train], y_train[train,0])\n",
        "        pred_val_x= ridge_x.predict(pred_train_x.loc[val])\n",
        "\n",
        "        ridge_y.fit(pred_train_y.loc[train], y_train[train,1])\n",
        "        pred_val_y= ridge_y.predict(pred_train_y.loc[val])\n",
        "\n",
        "        print(mean_euclidean_error(y_train[val], np.column_stack((pred_val_x, pred_val_y))))\n",
        "        val_mee.append(mean_euclidean_error(y_train[val], np.column_stack((pred_val_x, pred_val_y))))\n",
        "        \n",
        "      \n",
        "\n",
        "      return (np.mean(val_mee), np.std(val_mee), self.final_estimator_x.alpha_, self.final_estimator_y.alpha_)\n",
        "\n",
        "  #In the predict method we first retrain the base estimators on the whole TR\n",
        "  #And then we use the trained models to predict the TS, building the DataFrame on which Ridge predicts the final outputs\n",
        "  def predict(self, X_train, y_train, X_test):\n",
        "      pred_test_x= pd.DataFrame(columns=self.col)\n",
        "      pred_test_y= pd.DataFrame(columns=self.col)\n",
        "\n",
        "      for (name, estimator) in self.estimators: #modifico test\n",
        "        estimator.fit(X_train, y_train)\n",
        "        pred_test= estimator.predict(X_test)\n",
        "        pred_test_x[name]= pred_test[:,0]\n",
        "        pred_test_y[name]= pred_test[:,1]\n",
        "\n",
        "      return np.column_stack((self.final_estimator_x.predict(pred_test_x), self.final_estimator_y.predict(pred_test_y)))"
      ],
      "metadata": {
        "id": "kGITRBhfqLUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stk= Stacking()\n",
        "(mean, std, alpha1, alpha2)= stk.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "g3iNU39gA_Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stk= Stacking()\n",
        "(mean, std, alpha1, alpha2)= stk.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "zmtHl4zsrTVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mean, std, alpha1, alpha2)"
      ],
      "metadata": {
        "id": "vZV-JasiI08n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mean, std, alpha1, alpha2)"
      ],
      "metadata": {
        "id": "opAMTKi-rUXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train= stk.predict(x_train, y_train, x_train)"
      ],
      "metadata": {
        "id": "Gmf_T5jhS7FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_euclidean_error(y_train, y_pred_train)"
      ],
      "metadata": {
        "id": "lwB6nG-QS7NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test= stk.predict(x_train, y_train, x_test)"
      ],
      "metadata": {
        "id": "thGWg_3JVrdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_euclidean_error(y_test, y_pred_test)"
      ],
      "metadata": {
        "id": "MUc19rEfeNhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OyUkxnesZjXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Voting"
      ],
      "metadata": {
        "id": "0ZBJnlpApbjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Voting regressor is an ensemble which averages the individual predictions from base estimators to form a final prediction."
      ],
      "metadata": {
        "id": "Z_TNMLOD0vMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Voting():\n",
        "\n",
        "  #Voting constructor which takes the best grid's output hyper-parameters for\n",
        "  #the estimators to ensamble\n",
        "  def __init__(self) -> None:\n",
        "      nn_param_batch={\n",
        "          'weight_init': 0.4,\n",
        "          'weight_distr': 1,\n",
        "          'unit': 40,\n",
        "          'layer': 3,\n",
        "          'eta': 0.03,\n",
        "          'alpha': 0.95,\n",
        "          'lambd': 0.001,\n",
        "          'activ': 'tanh',\n",
        "      }\n",
        "      mean_err_batch= 0.8342830985784531\n",
        "      trainingStopCallback_batch = haltCallback(mean_err_batch)\n",
        "\n",
        "      nn_param_mb={\n",
        "          'weight_init': 0.2,\n",
        "          'weight_distr': 1,\n",
        "          'unit': 50,\n",
        "          'layer': 3,\n",
        "          'eta': 0.025,\n",
        "          'alpha': 0,\n",
        "          'lambd': 0.0005,\n",
        "          'activ': 'tanh'\n",
        "      }\n",
        "      mean_err_mb= 1.0076722502708435\n",
        "      trainingStopCallback_mb = haltCallback(mean_err_mb)\n",
        "\n",
        "\n",
        "      nn_param_adam={\n",
        "          'weight_init': 0.4,\n",
        "          'weight_distr': 1,\n",
        "          'unit': 40,\n",
        "          'layer': 3,\n",
        "          'eta': 0.025,\n",
        "          'beta_1': 0.9,\n",
        "          'beta_2': 0.7,\n",
        "          'lambd': 0.001,\n",
        "          'activ': 'tanh'\n",
        "      }\n",
        "      mean_err_adam= 0.8765210807323456\n",
        "      trainingStopCallback_adam = haltCallback(mean_err_adam)\n",
        "\n",
        "\n",
        "      nn_param_online={\n",
        "          'weight_init': 0.2,\n",
        "          'weight_distr': 1,\n",
        "          'unit': 50,\n",
        "          'eta': 0.0005,\n",
        "          'alpha': 0,\n",
        "          'layer': 2,\n",
        "          'lambd': 0.001,\n",
        "          'activ': 'tanh'\n",
        "      }\n",
        "      mean_err_online= 0.9664344638586044\n",
        "      trainingStopCallback_online = haltCallback(mean_err_online)\n",
        "\n",
        "      #estimator definition with best hyper-parameters\n",
        "      self.estimators=[\n",
        "            ('knn', KNeighborsRegressor(n_neighbors= 9, p=1, weights='distance')),\n",
        "            #('svm', MultiOutputRegressor(SVR(C= 10, epsilon= 0.5, gamma= 'auto', kernel= 'rbf'))),\n",
        "            #('lbe', Pipeline(steps=[('lbe',PolynomialFeatures(degree=4, interaction_only=True)),('ridge',Ridge(alpha=10, solver='saga', random_state=0))])),\n",
        "            ('batch', KerasRegressor(build_model, **nn_param_batch, epochs=800, shuffle=True, batch_size=len(x_train), verbose=0, callbacks=[trainingStopCallback_batch])),\n",
        "            #('mb', KerasRegressor(build_model, **nn_param_mb, epochs=800, shuffle=True, batch_size=100, verbose=0, callbacks=[trainingStopCallback_mb])),\n",
        "            ('adam', KerasRegressor(build_model_Adam, **nn_param_adam, epochs=800, shuffle=True, batch_size=len(x_train), verbose=0, callbacks=[trainingStopCallback_adam])),\n",
        "            #('online', KerasRegressor(build_model, **nn_param_online, epochs=400, shuffle=True, batch_size=1, verbose=0, callbacks=[trainingStopCallback_online])),\n",
        "            ('forest', RandomForestRegressor(max_depth=20, random_state=0, n_estimators=100, criterion='squared_error', min_samples_split=2, min_samples_leaf=2, \n",
        "                                    max_features = 3, bootstrap=False))\n",
        "      ]\n",
        "      \n",
        "      self.col= [name for (name, estimator) in self.estimators]\n",
        "\n",
        "  #In the fit method we first train each base estimator with a cross validated approch.\n",
        "  #Then we build a DataFrame made by base estimator predictions on which we predict using the mean voting (one for each target x/y)\n",
        "  def fit(self, X_train, y_train):\n",
        "      pred_train_x= pd.DataFrame(columns=self.col, index=np.arange(0,len(X_train)))\n",
        "      pred_train_y= pd.DataFrame(columns=self.col, index=np.arange(0,len(X_train)))\n",
        "\n",
        "      val_mee=[]\n",
        "      kf = KFold(n_splits=4, shuffle=True, random_state=0)\n",
        "      for train, val in kf.split(X_train):\n",
        "        for (name, estimator) in self.estimators:\n",
        "          estimator.fit(X_train[train], y_train[train])\n",
        "          pred_val= estimator.predict(X_train[val])\n",
        "\n",
        "          pred_train_x.loc[val, name]= pred_val[:,0]\n",
        "          pred_train_y.loc[val, name]= pred_val[:,1]\n",
        "        \n",
        "        val_mee.append(mean_euclidean_error(y_train[val], np.column_stack((np.mean(pred_train_x.loc[val], axis=1), np.mean(pred_train_y.loc[val], axis=1)))))\n",
        "\n",
        "\n",
        "      return (np.mean(val_mee), np.std(val_mee))\n",
        "\n",
        "  #In the predict method we first retrain the base estimators on the whole TR\n",
        "  #And then we use the trained models to predict the TS, building the DataFrame on which we take the mean\n",
        "  def predict(self, X_train, y_train, X_test):\n",
        "      pred_test_x= pd.DataFrame(columns=self.col)\n",
        "      pred_test_y= pd.DataFrame(columns=self.col)\n",
        "\n",
        "      for (name, estimator) in self.estimators:\n",
        "        estimator.fit(X_train, y_train)\n",
        "        pred_test= estimator.predict(X_test)\n",
        "        pred_test_x[name]= pred_test[:,0]\n",
        "        pred_test_y[name]= pred_test[:,1]\n",
        "\n",
        "      return np.column_stack((np.mean(pred_test_x.values, axis=1), np.mean(pred_test_y.values, axis=1)))"
      ],
      "metadata": {
        "id": "h-hpQYJIpmzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stk= Voting()\n",
        "(mean,std)= stk.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "vUM_Ft0wpmzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mean, std)"
      ],
      "metadata": {
        "id": "Xn9saANspmzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train= stk.predict(x_train, y_train, x_train)"
      ],
      "metadata": {
        "id": "wRj4yOgoQ0jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_euclidean_error(y_train, y_pred_train)"
      ],
      "metadata": {
        "id": "fVI67pTJRVLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test= stk.predict(x_train, y_train, x_test)"
      ],
      "metadata": {
        "id": "QdI__kUWpmzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_euclidean_error(y_test, y_pred_test)"
      ],
      "metadata": {
        "id": "vVsGXDMypmzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "icOqmko5pmzU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}